# Example config file for syncS3toS3 script in ws mode.

[main]
# debug > 0 spits out tons of debugging output
debug=0
# number of concurrent copy threads
nthreads=4
# mode to specify format of ids (default mode ws)
mode=ws
# mongo creds for source db
mongo_host=localhost
mongo_database = workspace
mongo_user = ws
mongo_pwd = BLAH
# mongo collection to query (usually s3_objects in ws mode, nodes in blobstore mode)
mongo_collection=s3_objects
# key field in collection (usually key in ws mode, id in blobstore mode)
mongo_keyfield=key
# datestamp for beginning date to start with
# (defaults to 'now', rewritten to 'now' at the end of script)
datefile=wssync.last
# log successful copied objects (also used to skip copying next run)
logfile=wssync.log
# objects that failed to copy (for whatever reason)
# this file is first read, then emptied, when the script is run
retryfile=wssync.retry
# remove log before starting copy
resetlog=0
# path to mc minio client
mcpath=/opt/mc/mc
# tmp dir used if a file exceeds sizelimit (rare for ws objects)
tmpdir=/mnt/backups/syncS3toS3
# above this limit will download to tmpdir and use mc to upload
sizelimit=5000000000
# use nondefault storage class if desired (optional, use S3 server default if omitted)
# storageclass=REDUCED_REDUNDANCY
# use --disable-multipart switch with `mc`
# (to make minio server compute MD5s on entire object instead of chunks)
# disable-multipart=0

[source]
# source S3 instance
bucket = ws
# destination S3 config used by boto (to test whether target object already exists)
url = https://storage.googleapis.com
accessKey = BLAH
secretKey = BLAH
region = us-central-1
# whether remote uses self-signed cert
insecure=0

[destination]
# destination S3 instance
# mcendpoint uses .mc/config.json labels
mcendpoint = dests3
bucket = ws
# destination S3 config used by boto (to test whether target object already exists)
url = https://storage.googleapis.com
accessKey = BLAH
secretKey = BLAH
region = us-central-1
# whether remote uses self-signed cert
insecure=0
